# Core dependencies
torch>=2.10.0  # Required for varlen_attn API
transformers>=4.30.0
datasets>=2.14.0
huggingface_hub>=0.16.0

# Training utilities
wandb>=0.15.0
tensorboard>=2.13.0

# Optimizers
lion-pytorch>=0.1.2
# Muon optimizer is native in PyTorch 2.6+, no external package needed

# Quantized optimizers (optional, for memory optimization)
bitsandbytes>=0.41.0

# Flash Linear Attention (required for GatedDeltaNet)
flash-linear-attention

# Native FP8 training via torchao (recommended, easy install)
torchao>=0.5.0

# Optional: FP8 training with Transformer Engine on H100/H200 (requires CUDA 12.6+)
# Install manually if needed: pip install transformer-engine[pytorch]>=2.0.0
# The native FP8 backend (optimization/fp8_native.py) works without this dependency.

